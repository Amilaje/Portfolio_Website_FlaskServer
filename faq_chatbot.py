import os
import re
from typing import List, Optional, Dict
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, Runnable
from pydantic import BaseModel, Field

# --- Pydantic DTO ì •ì˜ (app.pyì™€ ê³µìœ ) ---

class ChatQueryRequest(BaseModel):
    """í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸ ìš”ì²­ DTO"""
    query: str

class ChatResponse(BaseModel):
    """í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜í•  ì±—ë´‡ ì‘ë‹µ DTO"""
    response: str
    latencyMs: int = 0
    sourceDocuments: List[str] = Field(default_factory=list)

# íŒŒì¼ ë° ì†ŒìŠ¤ ì •ë³´
FAQ_FILE_NAME = "jinho_faq.txt"
RAG_SOURCES = [FAQ_FILE_NAME] 
FALLBACK_SOURCE = ["AI ê°œë°œì ê¸°ë³¸ ì •ë³´ (Fallback)"] # í´ë°± ë‹µë³€ì˜ ì¶œì²˜

# ----------------------------------------------------------------------------------
# ğŸ”‘ [CONSTANTS] ì‚¬ìš©ì ì •ë³´ (ì €ì¥ëœ ì •ë³´ í™œìš©)
# ----------------------------------------------------------------------------------

TECH_STACK = ['Python', 'Java', 'LangGraph', 'LangChain', 'YOLO', 'PyTorch', 'TensorFlow', 'Scikit-learn', 'NumPy', 'Pandas', 'Spring Boot', 'React', 'PyCharm', 'VSCode', 'IntelliJ IDEA', 'GitHub', 'AWS', 'Azure', 'Docker', 'Kubernetes', 'MSA', 'MSA EZ Modeling', 'Kafka']
CERTIFICATIONS = ['AICE Basic', 'AICE Associate', 'Computer Literacy Level 2', 'Deep Learning Application Ability Level 3', 'IT Plus Level 2', 'Word Processor', 'ë¹…ë°ì´í„° ë¶„ì„ê¸°ì‚¬ (í•„ê¸° í•©ê²©)']
PROJECTS = ['Meteorological Administration Data Contest  Heat Demand Prediction Model (Awarded)', 'KT Aivle School Big Project â€“ Pharmacy Automation Platform (YeahYak)', 'MSA-based Book Subscription Platform Construction Project', 'Spring Boot + React-based Book Management Web System', 'AWS-based Cloud Infrastructure Construction', 'LLM Agent-based AI Interviewer Agent System', 'Machine Learning-based Motion Classification System', 'Image Classification (MobileNetV2-based)', 'Unity-based Game Development']
CONTACT_ME_GMAIL = "choijinho321@gmail.com"
CONTACT_ME_EMAIL = "choijinho123@naver.com"
CONTACT_Telephone = "010-7115-5860"
# ----------------------------------------------------------------------------------
# ğŸ’¡ [NEW/MODIFIED] 1. Fallback ì •ë³´ ë° ì—°ë½ì²˜ ì•ˆë‚´ ìƒì„±
# ----------------------------------------------------------------------------------
def get_fallback_info() -> str:
    """ì‚¬ìš©ìì˜ ì €ì¥ëœ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    # ... (ê¸°ì¡´ ì •ë³´ ìœ ì§€) ...
    project_list = "\n".join([f"- {p}" for p in PROJECTS])
    tech_stack_list = ", ".join(TECH_STACK)
    cert_list = ", ".join(CERTIFICATIONS)

    return f"""
    ### ê¸°ë³¸ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ (RAG ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš©)
    - **ì†Œê°œ:** AI ê°œë°œìë¡œ ì „ê³µ(ì²œë¬¸í•™)ì„ ì „í™˜í–ˆìœ¼ë©°, í˜„ì¬ KT Aivle School ì„ ìˆ˜íš¨í–ˆìŠµë‹ˆë‹¤.
    - **ëª©í‘œ:** í˜‘ì—… ë° ì‹¤í–‰ ëŠ¥ë ¥ì„ ê°–ì¶˜ ë¬¸ì œ í•´ê²° ì¤‘ì‹¬ì˜ ì‹¤ë¬´í˜• AI ê°œë°œìì…ë‹ˆë‹¤.
    - **ê¸°ìˆ  ìŠ¤íƒ:** {tech_stack_list}
    - **ì£¼ìš” í”„ë¡œì íŠ¸:** {project_list}
    - **ìê²©ì¦:** {cert_list}
    """

def get_contact_fallback_response(latency: int = 0) -> ChatResponse:
    """RAGì—ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ, ì—°ë½ì²˜ë¥¼ í¬í•¨í•œ ì•ˆë‚´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    contact_info = (
        f"**í˜„ì¬ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ë¬¸ì„œ ë‚´ì—ì„œëŠ” ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.** "
        f"í˜¹ì‹œ ê¸°íƒ€ ë¬¸ì˜ ì‚¬í•­ì´ë‚˜ ê°œì¸ì ì¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë‹¤ë©´, "
        f"**ë‹¤ìŒ ì—°ë½ì²˜ë¥¼ í†µí•´ AI ê°œë°œìë‹˜ê»˜ ì§ì ‘ ì—°ë½í•´ ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.**\n\n"
        f"â–¶ **E-mail**: {CONTACT_ME_EMAIL}\n"
        f"â–¶ **Telephone**: {CONTACT_Telephone}\n"
    )
    return ChatResponse(
        response=contact_info,
        latencyMs=latency,
        sourceDocuments=FALLBACK_SOURCE
    )

# ----------------------------------------------------------------------------------
# 2. Fallback ì²´ì¸ ì •ì˜ (ì‚¬ìš©í•˜ì§€ ì•Šê³ , get_contact_fallback_response ì‚¬ìš©)
# ----------------------------------------------------------------------------------
# def create_fallback_chain(llm: ChatOpenAI) -> Runnable:
#     """RAG ì‹¤íŒ¨ ì‹œ Fallback ë¡œì§ì´ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ í•¨ìˆ˜ëŠ” ë ˆê±°ì‹œë¡œ ë‚¨ê²¨ë‘ê±°ë‚˜ ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
#     # í˜„ì¬ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì´ ë³µì¡í•œ ì²´ì¸ ëŒ€ì‹  get_contact_fallback_responseë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#     return None 

# ----------------------------------------------------------------------------------

class FAQChatbot:
    """í¬íŠ¸í´ë¦¬ì˜¤ FAQì— ê¸°ë°˜í•œ RAG ì±—ë´‡ í´ë˜ìŠ¤"""

    def __init__(self):
        self.rag_chain: Optional[Runnable] = None
        # self.fallback_chain: Optional[Runnable] = None # Fallback ì²´ì¸ ì‚¬ìš© ì•ˆ í•¨
        self.initialize_rag_chain()
        self.contact_keywords = ["ì—°ë½", "ì´ë©”ì¼", "ì „í™”", "ë²ˆí˜¸", "email", "contact"] 


    def initialize_rag_chain(self):
        """RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("--- RAG ì²´ì¸ ì´ˆê¸°í™” ì‹œì‘ (faq_chatbot.py) ---")

        if not os.getenv("OPENAI_API_KEY"):
            print("ERROR: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            # 1~3. ë°ì´í„° ë¡œë“œ, ì²­í‚¹, ë²¡í„° ì €ì¥ì†Œ ë° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
            loader = TextLoader(FAQ_FILE_NAME, encoding="utf-8")
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            texts = text_splitter.split_documents(documents)
            print(f"ì´ {len(texts)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ìƒì„±ë¨.")
            embeddings = OpenAIEmbeddings()
            vectorstore = Chroma.from_documents(texts, embeddings)
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

            # 4. LLM ë° í”„ë¡¬í”„íŠ¸ ì •ì˜
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
            
            # RAG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Fallback ê°ì§€ ë¬¸êµ¬ ìœ ì§€)
            system_message = (
                "ë‹¹ì‹ ì€ AI ê°œë°œìë‹˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. "
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ ë‚´ì—ì„œë§Œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
                "ë§Œì•½ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì œê³µëœ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ ë‚´ì—ì„œëŠ” ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ì‘ë‹µí•˜ì‹­ì‹œì˜¤. "
                "ë‹µë³€ ì‹œ 1ì¸ì¹­ ëŒ€ëª…ì‚¬('ì €', 'ì €ì˜')ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•´ë„ ì¢‹ìŠµë‹ˆë‹¤. "
                "ë°˜ë“œì‹œ ê³µì†í•˜ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì‹­ì‹œì˜¤."
            )

            prompt = ChatPromptTemplate.from_messages(
                [
                    SystemMessagePromptTemplate.from_template(system_message),
                    HumanMessagePromptTemplate.from_template(
                        "ì§ˆë¬¸: {question}\n\nì°¸ê³  ë¬¸ì„œ:\n{context}"
                    ),
                ]
            )

            # 5. RAG ì²´ì¸ ì¡°í•©
            self.rag_chain = (
                {"context": retriever, "question": RunnablePassthrough()}
                | prompt
                | llm
                | StrOutputParser()
            )

            print("--- RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ ---")

        except Exception as e:
            print(f"ERROR: RAG ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.rag_chain = None
            
    # ----------------------------------------------------------------------------------
    # 3. íŠ¹ìˆ˜ ì¿¼ë¦¬ ì²˜ë¦¬ ë¡œì§ (ì—°ë½ì²˜ ì •ë³´ get_contact_fallback_response í•¨ìˆ˜ ì‚¬ìš©)
    # ----------------------------------------------------------------------------------
    def handle_special_query(self, query: str) -> Optional[ChatResponse]:
        """ì—°ë½ì²˜ ìš”ì²­ ë“± íŠ¹ìˆ˜ ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        
        query_lower = query.lower()

        # 1. ì—°ë½ì²˜ ìš”ì²­ ì²˜ë¦¬
        if not query or any(keyword in query_lower for keyword in self.contact_keywords):
            # íŠ¹ìˆ˜ ì¿¼ë¦¬ ì‘ë‹µë„ get_contact_fallback_response í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í†µì¼
            return get_contact_fallback_response(latency=0)
        
        # 2. ë‹¨ìˆœ ì¸ì‚¬ë§ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        greeting_keywords = ["ì•ˆë…•", "ë°˜ê°€ì›Œ", "ì†Œê°œ", "í—¬ë¡œ", "hi", "ì¸ì‚¬"]
        if any(keyword in query_lower for keyword in greeting_keywords):
            return ChatResponse(
                response="ë°˜ê°‘ìŠµë‹ˆë‹¤! ì €ëŠ” AI ê°œë°œìë‹˜ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì–´ë–¤ í”„ë¡œì íŠ¸ë‚˜ ê¸°ìˆ  ìŠ¤íƒì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                latencyMs=0,
                sourceDocuments=[]
            )
        
        return None

    # ----------------------------------------------------------------------------------
    # 4. ìµœì¢… ì‘ë‹µ ìƒì„± ë¡œì§ (RAG ì‹¤íŒ¨ ì‹œ ì—°ë½ì²˜ Fallback)
    # ----------------------------------------------------------------------------------
    def get_rag_response(self, query: str) -> ChatResponse:
        """RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì—°ë½ì²˜ ì•ˆë‚´ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        # 1. RAG ì²´ì¸ ì‹¤í–‰
        rag_response = self.rag_chain.invoke(query)
        
       # 2. RAG ì‹¤íŒ¨ ê°ì§€ ë¡œì§ ê°œì„ :
        # "ì •ë³´ ë‚´ì—ì„œëŠ” ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë‚˜ "ì •ë³´ ë‚´ì—ì„œëŠ” ~~ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." ëª¨ë‘ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
        if "ì •ë³´ ë‚´ì—ì„œëŠ”" in rag_response and ("ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in rag_response or "ì—†ìŠµë‹ˆë‹¤" in rag_response):
            print("INFO: RAG ì •ë³´ ì°¾ê¸° ì‹¤íŒ¨. ì—°ë½ì²˜ Fallback ì‘ë‹µ ì œê³µ.")
            
            # 3. Fallback: ì—°ë½ì²˜ ì •ë³´ê°€ í¬í•¨ëœ ChatResponse ê°ì²´ ë°˜í™˜
            return get_contact_fallback_response(latency=0) 
        
        # 4. RAG ì„±ê³µ ì‹œ, ChatResponse ê°ì²´ ë°˜í™˜
        return ChatResponse(
            response=rag_response,
            latencyMs=0, 
            sourceDocuments=RAG_SOURCES
        )